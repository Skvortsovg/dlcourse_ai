{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08975373, -0.03568431,  0.08994824, ..., -0.03998549,\n",
       "         0.09710392,  1.        ],\n",
       "       [ 0.01612863,  0.01921765,  0.05465412, ..., -0.13410314,\n",
       "        -0.10681765,  1.        ],\n",
       "       [-0.25445961, -0.32980196, -0.36887529, ..., -0.20469137,\n",
       "        -0.23230784,  1.        ],\n",
       "       ...,\n",
       "       [ 0.38083451,  0.36823725,  0.36053647, ...,  0.23060275,\n",
       "         0.21867255,  1.        ],\n",
       "       [-0.2426949 , -0.22391961, -0.12965961, ..., -0.14194627,\n",
       "        -0.13034706,  1.        ],\n",
       "       [ 0.01220706,  0.02706078,  0.18798745, ...,  0.06981843,\n",
       "         0.10886863,  1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "from gradient_check import check_gradient\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import linear_classifer\n",
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import linear_classifer\n",
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import linear_classifer\n",
    "\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import linear_classifer\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397687\n",
      "Epoch 1, loss: 2.330513\n",
      "Epoch 2, loss: 2.311144\n",
      "Epoch 3, loss: 2.304570\n",
      "Epoch 4, loss: 2.303160\n",
      "Epoch 5, loss: 2.303056\n",
      "Epoch 6, loss: 2.301780\n",
      "Epoch 7, loss: 2.302396\n",
      "Epoch 8, loss: 2.301589\n",
      "Epoch 9, loss: 2.300945\n"
     ]
    }
   ],
   "source": [
    "import linear_classifer\n",
    "\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f94df6c59e8>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.125\n",
      "Epoch 0, loss: 2.301629\n",
      "Epoch 1, loss: 2.301919\n",
      "Epoch 2, loss: 2.302523\n",
      "Epoch 3, loss: 2.301311\n",
      "Epoch 4, loss: 2.302054\n",
      "Epoch 5, loss: 2.302173\n",
      "Epoch 6, loss: 2.302712\n",
      "Epoch 7, loss: 2.302095\n",
      "Epoch 8, loss: 2.302179\n",
      "Epoch 9, loss: 2.302007\n",
      "Epoch 10, loss: 2.301464\n",
      "Epoch 11, loss: 2.301976\n",
      "Epoch 12, loss: 2.301795\n",
      "Epoch 13, loss: 2.301439\n",
      "Epoch 14, loss: 2.302229\n",
      "Epoch 15, loss: 2.302286\n",
      "Epoch 16, loss: 2.303321\n",
      "Epoch 17, loss: 2.302511\n",
      "Epoch 18, loss: 2.302486\n",
      "Epoch 19, loss: 2.302873\n",
      "Epoch 20, loss: 2.301801\n",
      "Epoch 21, loss: 2.301355\n",
      "Epoch 22, loss: 2.301825\n",
      "Epoch 23, loss: 2.302187\n",
      "Epoch 24, loss: 2.302101\n",
      "Epoch 25, loss: 2.302480\n",
      "Epoch 26, loss: 2.302529\n",
      "Epoch 27, loss: 2.301585\n",
      "Epoch 28, loss: 2.301976\n",
      "Epoch 29, loss: 2.301327\n",
      "Epoch 30, loss: 2.301778\n",
      "Epoch 31, loss: 2.301380\n",
      "Epoch 32, loss: 2.302286\n",
      "Epoch 33, loss: 2.302310\n",
      "Epoch 34, loss: 2.301027\n",
      "Epoch 35, loss: 2.302436\n",
      "Epoch 36, loss: 2.301471\n",
      "Epoch 37, loss: 2.302226\n",
      "Epoch 38, loss: 2.301985\n",
      "Epoch 39, loss: 2.302118\n",
      "Epoch 40, loss: 2.302319\n",
      "Epoch 41, loss: 2.301310\n",
      "Epoch 42, loss: 2.302062\n",
      "Epoch 43, loss: 2.301059\n",
      "Epoch 44, loss: 2.302161\n",
      "Epoch 45, loss: 2.302292\n",
      "Epoch 46, loss: 2.301699\n",
      "Epoch 47, loss: 2.301763\n",
      "Epoch 48, loss: 2.302925\n",
      "Epoch 49, loss: 2.301255\n",
      "Epoch 50, loss: 2.301974\n",
      "Epoch 51, loss: 2.302867\n",
      "Epoch 52, loss: 2.302130\n",
      "Epoch 53, loss: 2.302244\n",
      "Epoch 54, loss: 2.301736\n",
      "Epoch 55, loss: 2.302353\n",
      "Epoch 56, loss: 2.301920\n",
      "Epoch 57, loss: 2.301715\n",
      "Epoch 58, loss: 2.302309\n",
      "Epoch 59, loss: 2.300979\n",
      "Epoch 60, loss: 2.301968\n",
      "Epoch 61, loss: 2.300936\n",
      "Epoch 62, loss: 2.301303\n",
      "Epoch 63, loss: 2.301715\n",
      "Epoch 64, loss: 2.302649\n",
      "Epoch 65, loss: 2.301957\n",
      "Epoch 66, loss: 2.301272\n",
      "Epoch 67, loss: 2.302043\n",
      "Epoch 68, loss: 2.301934\n",
      "Epoch 69, loss: 2.301685\n",
      "Epoch 70, loss: 2.301981\n",
      "Epoch 71, loss: 2.303228\n",
      "Epoch 72, loss: 2.301737\n",
      "Epoch 73, loss: 2.301979\n",
      "Epoch 74, loss: 2.302648\n",
      "Epoch 75, loss: 2.301898\n",
      "Epoch 76, loss: 2.301231\n",
      "Epoch 77, loss: 2.301473\n",
      "Epoch 78, loss: 2.300880\n",
      "Epoch 79, loss: 2.302065\n",
      "Epoch 80, loss: 2.302652\n",
      "Epoch 81, loss: 2.301606\n",
      "Epoch 82, loss: 2.302547\n",
      "Epoch 83, loss: 2.300664\n",
      "Epoch 84, loss: 2.302306\n",
      "Epoch 85, loss: 2.302206\n",
      "Epoch 86, loss: 2.301915\n",
      "Epoch 87, loss: 2.302177\n",
      "Epoch 88, loss: 2.301958\n",
      "Epoch 89, loss: 2.301698\n",
      "Epoch 90, loss: 2.301637\n",
      "Epoch 91, loss: 2.301861\n",
      "Epoch 92, loss: 2.301170\n",
      "Epoch 93, loss: 2.302221\n",
      "Epoch 94, loss: 2.301576\n",
      "Epoch 95, loss: 2.301660\n",
      "Epoch 96, loss: 2.302076\n",
      "Epoch 97, loss: 2.301679\n",
      "Epoch 98, loss: 2.302099\n",
      "Epoch 99, loss: 2.302444\n",
      "Accuracy after training for 100 epochs:  0.127\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import linear_classifer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LinearSoftmaxClassifier(batch_size=100, epochs=1, learning_rate=1e-07,\n",
       "            reg=1e-05),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'learning_rate': [0.001, 0.0001, 1e-05], 'reg': [0.0001, 1e-05, 1e-06], 'epochs': [10], 'batch_size': [300]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "        \n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "parameters = {'learning_rate': learning_rates, \n",
    "              'reg': reg_strengths,\n",
    "              'epochs': [num_epochs],\n",
    "              'batch_size': [batch_size]\n",
    "             }\n",
    "clf = linear_classifer.LinearSoftmaxClassifier()\n",
    "gscv = GridSearchCV(clf, parameters, cv=5)\n",
    "gscv.fit(train_X, train_y)\n",
    "\n",
    "#print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16433333333333333"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear classifier with l = 1.000000e-03 and r = 1.000000e-04\n",
      "Accuracy: 0.20\n",
      "Linear classifier with l = 1.000000e-03 and r = 1.000000e-05\n",
      "Accuracy: 0.19\n",
      "Linear classifier with l = 1.000000e-03 and r = 1.000000e-06\n",
      "Accuracy: 0.20\n",
      "Linear classifier with l = 1.000000e-04 and r = 1.000000e-04\n",
      "Accuracy: 0.14\n",
      "Linear classifier with l = 1.000000e-04 and r = 1.000000e-05\n",
      "Accuracy: 0.14\n",
      "Linear classifier with l = 1.000000e-04 and r = 1.000000e-06\n",
      "Accuracy: 0.13\n",
      "Linear classifier with l = 1.000000e-05 and r = 1.000000e-04\n",
      "Accuracy: 0.10\n",
      "Linear classifier with l = 1.000000e-05 and r = 1.000000e-05\n",
      "Accuracy: 0.10\n",
      "Linear classifier with l = 1.000000e-05 and r = 1.000000e-06\n",
      "Accuracy: 0.10\n",
      "l, r = 0.0001, 0.0001, accuracy = 0.139667\n",
      "l, r = 0.0001, 1e-05, accuracy = 0.140000\n",
      "l, r = 0.0001, 1e-06, accuracy = 0.130222\n",
      "l, r = 0.001, 0.0001, accuracy = 0.196444\n",
      "l, r = 0.001, 1e-05, accuracy = 0.192778\n",
      "l, r = 0.001, 1e-06, accuracy = 0.196000\n",
      "l, r = 1e-05, 0.0001, accuracy = 0.099889\n",
      "l, r = 1e-05, 1e-05, accuracy = 0.104778\n",
      "l, r = 1e-05, 1e-06, accuracy = 0.101111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = -1\n",
    "\n",
    "# Find the best k using cross-validation based on accuracy\n",
    "num_folds = 5\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "l_r_to_accuracy = {}\n",
    "\n",
    "# TODO: split the training data in 5 folds and store them in train_folds_X/train_folds_y\n",
    "\n",
    "kf = KFold(num_folds, shuffle=True)\n",
    "\n",
    "for train, test in kfold.split(train_X):\n",
    "    train_folds.append(train)\n",
    "    test_folds.append(test)  \n",
    "\n",
    "for l in learning_rates:\n",
    "    for r in reg_strengths:\n",
    "        fix_l_r_accuracy = []\n",
    "        \n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier(\n",
    "            epochs=num_epochs, \n",
    "            learning_rate=l, \n",
    "            batch_size=batch_size, \n",
    "            reg=r)\n",
    "        for i, train in enumerate(train_folds_X):\n",
    "            loss_history = classifier.fit(train_X[train], train_y[train])\n",
    "            pred = classifier.predict(train_X[test_folds_X[i]])\n",
    "            accuracy = multiclass_accuracy(pred, train_y[test_folds_X[i]])\n",
    "            fix_l_r_accuracy.append(accuracy)\n",
    "\n",
    "        l_r_to_accuracy[str(l) + ', ' + str(r)] = np.average(fix_l_r_accuracy)\n",
    "        print(\"Linear classifier with l = %e and r = %e\" % (l, r))\n",
    "        print(\"Accuracy: %4.2f\" % (l_r_to_accuracy[str(l) + ', ' + str(r)])) \n",
    "        if l_r_to_accuracy[str(l) + ', ' + str(r)] > best_val_accuracy:\n",
    "            best_val_accuracy = l_r_to_accuracy[str(l) + ', ' + str(r)]\n",
    "            best_classifier = classifier\n",
    "\n",
    "for lr in sorted(l_r_to_accuracy):\n",
    "    print('l, r = %s, accuracy = %f' % (lr, l_r_to_accuracy[lr]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.184000\n"
     ]
    }
   ],
   "source": [
    "test_pred = gscv.best_estimator_.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.163000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
